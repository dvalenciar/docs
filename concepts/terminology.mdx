---
title: "Reinforcement Learning Key Terminology"
sidebarTitle: "Terminology"
---

This section presents the key mathematical foundations and fundamental concepts that underpin RL. It covers specialized techniques and equations essential for understanding how agents learn, make decisions, and optimize their behavior. These principles will serve as the theoretical basis for the following sections.

<Tip>
  It is completely normal to need multiple readings to grasp these concepts fully. If you find them a bit confusing at first, that's perfectly okay—everyone experiences this. Don't get discouraged! With time and practice, these ideas will become much clearer. Keep going!
</Tip>

## <Icon icon="brain-circuit" />  Policy&#x20;

A **policy** is essentially the strategy an agent uses to decide what actions to take in different situations. It tells the agent what to do when it's in a specific state. Policies are typically divided into two types:

1. **Stochastic Policy**: This type of policy involves randomness. Instead of choosing a single action, the agent picks from a set of actions based on probabilities. For example, the agent might have a 70% chance of choosing action A and a 30% chance of choosing action B.

2. **Deterministic Policy**: With this policy, the agent always chooses the same action for a given state. There's no randomness—each state is mapped to exactly one action.

<Icon icon="brain-circuit" /> Value Functions

In simple terms, value functions help estimate how good it is for an agent to be in a certain state or to take a specific action from that state. They estimate the expected total return, $G\_t$​, an agent can receive over time. The return is the total sum of rewards the agent collects during its journey, while a reward is the immediate feedback after taking an action.

There are two types of value functions:

1. **State-Value Function** $V^{\pi}(s)$​: This estimates the expected return the agent will receive if it starts in state $s$ and follows a policy  $\pi$.

2. **Action-Value Function** (or **Q-value**) $Q^{\pi}(s)$​: This estimates the expected return if the agent takes a specific action $a$ from state $s$ and then follows a policy $\pi$.